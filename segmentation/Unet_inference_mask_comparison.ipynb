{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d67889ab-b8ce-4eb0-ab38-f90d4f7a7b5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:98% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /usr/lib/python3/dist-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:98% !important; }</style>\"))\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "\n",
    "import training\n",
    "from training import load_config, generate_dataloader, get_model, train_loop, cal_MeanIoU_score\n",
    "from utils.preprocessing import load_image, apply_img_preprocessing\n",
    "\n",
    "from inference import load_saved_model, pred_segmentation_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70897bf4-de7c-4973-aa4a-a06fd03487df",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_weight_path = \"experiment_results/checkpoints/unet_checkpoint_epoch_1.pth\"\n",
    "\n",
    "model_name = \"unet\"\n",
    "model_config = {'in_channels': 3, 'out_channels': 1}\n",
    "inference_config = {'foreground_threshold': 0.5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "611ebace-39e8-436d-bab2-942a07a7a23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update preprocessing according to training\n",
    "resize_height, resize_width = 360, 640\n",
    "\n",
    "# Define the image transformations\n",
    "img_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((resize_height, resize_width)),   # ensure resize is same as used during training for loaded model \n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e202d6f-b776-4717-ba92-e83cb2a9c959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using DEVICE: cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using DEVICE: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfd097cd-199b-4f68-a4ce-7f240baa7291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved weights loaded\n",
      "Saved weights loaded\n",
      "Saved weights loaded\n",
      "Saved weights loaded\n",
      "Saved weights loaded\n",
      "Saved weights loaded\n"
     ]
    }
   ],
   "source": [
    "# initialize and load saved model\n",
    "model1 = load_saved_model(model_name=model_name, saved_weight_path=\"experiment_results/checkpoints/unet_checkpoint_epoch_1.pth\", **model_config)\n",
    "model2 = load_saved_model(model_name=model_name, saved_weight_path=\"experiment_results/checkpoints/unet_checkpoint_epoch_2.pth\", **model_config)\n",
    "model3 = load_saved_model(model_name=model_name, saved_weight_path=\"experiment_results/checkpoints/unet_checkpoint_epoch_3.pth\", **model_config)\n",
    "model4 = load_saved_model(model_name=model_name, saved_weight_path=\"experiment_results/checkpoints/unet_checkpoint_epoch_4.pth\", **model_config)\n",
    "model5 = load_saved_model(model_name=model_name, saved_weight_path=\"experiment_results/checkpoints/unet_checkpoint_epoch_5.pth\", **model_config)\n",
    "model6 = load_saved_model(model_name=model_name, saved_weight_path=\"experiment_results/checkpoints/unet_checkpoint_epoch_6.pth\", **model_config)\n",
    "model1 = model.to(DEVICE)\n",
    "model2 = model.to(DEVICE)\n",
    "model3 = model.to(DEVICE)\n",
    "model4 = model.to(DEVICE)\n",
    "model5 = model.to(DEVICE)\n",
    "model6 = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81d9e791-43d6-41a2-a028-0e1b37b734dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = \"/cs6945share/retro_project/bdd100k/images/val/\"\n",
    "img_paths = glob(img_dir + \"*.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee1695b2-6a41-49f3-8fbb-c4f45e6450a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6afaa6ee2585448e8a9cbcd8818b3d3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Dropdown(description='File:', index=30, layout=Layout(width='600px'), options=('â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "img_indice=30\n",
    "\n",
    "# Dropdown for file selection\n",
    "file_dropdown = widgets.Dropdown(\n",
    "    options=img_paths,\n",
    "    value=img_paths[img_indice],\n",
    "    description='File:',\n",
    "    layout=widgets.Layout(width='600px')  # Adjust width to suit your preference\n",
    ")\n",
    "\n",
    "\n",
    "# Two images side by side\n",
    "# Output widget to hold the matplotlib figure\n",
    "image_output = widgets.Output()\n",
    "\n",
    "# === Image Update Function ===\n",
    "def update_images(change=None):\n",
    "    selected_file = file_dropdown.value\n",
    "    # full_path = os.path.join(image_folder, selected_file)\n",
    "    img = Image.open(selected_file)\n",
    "    mask = Image.open(selected_file.replace(\"images\", \"generated_masks_v0_2\").replace(\".jpg\", \".png\"))\n",
    "    loaded_img = load_image(selected_file)\n",
    "    # Model 1:\n",
    "    pred_mask1 = pred_segmentation_mask(\n",
    "        model=model1, test_img=loaded_img, img_transform=img_transform, add_batch_dim=True, device=DEVICE, \n",
    "        pos_threshold=inference_config[\"foreground_threshold\"])\n",
    "    # Convert the model output to a PIL Image (if it's float/binary, map to [0..255] as needed)\n",
    "    pred_mask_pil1 = Image.fromarray((np.squeeze(pred_mask1) * 255).astype(np.uint8))\n",
    "    # # Model 2:\n",
    "    # pred_mask2 = pred_segmentation_mask(\n",
    "    #     model=model2, test_img=loaded_img, img_transform=img_transform, add_batch_dim=True, device=DEVICE, \n",
    "    #     pos_threshold=inference_config[\"foreground_threshold\"])\n",
    "    # # Convert the model output to a PIL Image (if it's float/binary, map to [0..255] as needed)\n",
    "    # pred_mask_pil2 = Image.fromarray((np.squeeze(pred_mask2) * 255).astype(np.uint8))\n",
    "    # # Model 3:\n",
    "    # pred_mask3 = pred_segmentation_mask(\n",
    "    #     model=model3, test_img=loaded_img, img_transform=img_transform, add_batch_dim=True, device=DEVICE, \n",
    "    #     pos_threshold=inference_config[\"foreground_threshold\"])\n",
    "    # # Convert the model output to a PIL Image (if it's float/binary, map to [0..255] as needed)\n",
    "    # pred_mask_pil3 = Image.fromarray((np.squeeze(pred_mask3) * 255).astype(np.uint8))\n",
    "    # # Model 4:\n",
    "    # pred_mask4 = pred_segmentation_mask(\n",
    "    #     model=model4, test_img=loaded_img, img_transform=img_transform, add_batch_dim=True, device=DEVICE, \n",
    "    #     pos_threshold=inference_config[\"foreground_threshold\"])\n",
    "    # # Convert the model output to a PIL Image (if it's float/binary, map to [0..255] as needed)\n",
    "    # pred_mask_pil4 = Image.fromarray((np.squeeze(pred_mask4) * 255).astype(np.uint8))\n",
    "    # # Model 5:\n",
    "    # pred_mask5 = pred_segmentation_mask(\n",
    "    #     model=model5, test_img=loaded_img, img_transform=img_transform, add_batch_dim=True, device=DEVICE, \n",
    "    #     pos_threshold=inference_config[\"foreground_threshold\"])\n",
    "    # # Convert the model output to a PIL Image (if it's float/binary, map to [0..255] as needed)\n",
    "    # pred_mask_pil5 = Image.fromarray((np.squeeze(pred_mask5) * 255).astype(np.uint8))\n",
    "    # # Model 6:\n",
    "    # pred_mask6 = pred_segmentation_mask(\n",
    "    #     model6=model, test_img=loaded_img, img_transform=img_transform, add_batch_dim=True, device=DEVICE, \n",
    "    #     pos_threshold=inference_config[\"foreground_threshold\"])\n",
    "    # # Convert the model output to a PIL Image (if it's float/binary, map to [0..255] as needed)\n",
    "    # pred_mask_pil6 = Image.fromarray((np.squeeze(pred_mask6) * 255).astype(np.uint8))\n",
    "\n",
    "    # Resize to match original image dimensions\n",
    "    pred_mask_resized1 = pred_mask_pil1.resize(img.size, resample=Image.NEAREST)\n",
    "    # pred_mask_resized2 = pred_mask_pil2.resize(img.size, resample=Image.NEAREST)\n",
    "    # pred_mask_resized3 = pred_mask_pil3.resize(img.size, resample=Image.NEAREST)\n",
    "    # pred_mask_resized4 = pred_mask_pil4.resize(img.size, resample=Image.NEAREST)\n",
    "    # pred_mask_resized5 = pred_mask_pil5.resize(img.size, resample=Image.NEAREST)\n",
    "\n",
    "    with image_output:\n",
    "        clear_output(wait=True)\n",
    "        scale = 2\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(16*scale,8*scale))  # side-by-side\n",
    "        axs[0].imshow(img)\n",
    "        axs[0].imshow(mask, cmap=\"Reds\", alpha=0.5)\n",
    "        axs[0].set_title('Annotations')\n",
    "        axs[0].axis('off')\n",
    "        \n",
    "        axs[1].imshow(img)\n",
    "        axs[1].imshow(pred_mask_resized1, cmap=\"Reds\", alpha=0.5)\n",
    "        axs[1].set_title('Segmentation Model ChkPt #1 Predictions')\n",
    "        axs[1].axis('off')\n",
    "        \n",
    "        # axs[1, 0].imshow(img)\n",
    "        # axs[1, 0].imshow(pred_mask_resized2, cmap=\"Reds\", alpha=0.5)\n",
    "        # axs[1, 0].set_title('Segmentation Model ChkPt #2 Predictions')\n",
    "        # axs[1, 0].axis('off')\n",
    "        \n",
    "        # axs[1, 1].imshow(img)\n",
    "        # axs[1, 1].imshow(pred_mask_resized3, cmap=\"Reds\", alpha=0.5)\n",
    "        # axs[1, 1].set_title('Segmentation Model ChkPt #3 Predictions')\n",
    "        # axs[1, 1].axis('off')\n",
    "        \n",
    "        # axs[2, 0].imshow(img)\n",
    "        # axs[2, 0].imshow(pred_mask_resized4, cmap=\"Reds\", alpha=0.5)\n",
    "        # axs[2, 0].set_title('Segmentation Model ChkPt #4 Predictions')\n",
    "        # axs[2, 0].axis('off')\n",
    "        \n",
    "        # axs[2, 1].imshow(img)\n",
    "        # axs[2, 1].imshow(pred_mask_resized5, cmap=\"Reds\", alpha=0.5)\n",
    "        # axs[2, 1].set_title('Segmentation Model ChkPt #5 Predictions')\n",
    "        # axs[2, 1].axis('off')\n",
    "        plt.show()\n",
    "\n",
    "update_images()\n",
    "\n",
    "file_dropdown.observe(update_images, names=\"value\")\n",
    "\n",
    "# Navigation buttons\n",
    "prev_button = widgets.Button(description='Prev', button_style='')\n",
    "next_button = widgets.Button(description='Next', button_style='')\n",
    "\n",
    "def on_prev_clicked(b):\n",
    "    current_index = img_paths.index(file_dropdown.value)\n",
    "    new_index = (current_index - 1) % len(img_paths)  # wrap around\n",
    "    file_dropdown.value = img_paths[new_index]\n",
    "\n",
    "def on_next_clicked(b):\n",
    "    current_index = img_paths.index(file_dropdown.value)\n",
    "    new_index = (current_index + 1) % len(img_paths)  # wrap around\n",
    "    file_dropdown.value = img_paths[new_index]\n",
    "\n",
    "prev_button.on_click(on_prev_clicked)\n",
    "next_button.on_click(on_next_clicked)\n",
    "\n",
    "# Lay out the widgets\n",
    "# Top bar (file dropdown)\n",
    "top_bar = widgets.HBox([file_dropdown],\n",
    "                        layout=widgets.Layout(justify_content='center',margin='10px 0'))\n",
    "\n",
    "# Bottom bar (prev and next buttons)\n",
    "bottom_bar = widgets.HBox([prev_button, next_button],\n",
    "                          layout=widgets.Layout(justify_content='center',margin='10px 0'))\n",
    "\n",
    "# Combine everything in a vertical box\n",
    "gui = widgets.VBox([top_bar, image_output, bottom_bar])\n",
    "\n",
    "display(gui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428e7585-2a51-46ba-b5a9-68a4a7297ff2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
